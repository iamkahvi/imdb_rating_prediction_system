{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b847853d",
   "metadata": {},
   "source": [
    "# Decision Trees using hand-written ID3\n",
    "\n",
    "About 60% of the code here is the same as the ID3 implementation from Assignment one. Many parts of the algorithm had to be adapted to support an arbitrary number of output classes instead of just the true/false that the assignment needed.\n",
    "\n",
    "The evaluation function was also updated to compute the MAE instead of simply the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996883b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "import pprint\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Most of the code here is based on my ID3 implementation\n",
    "# from assignment one. This is cited in the report.\n",
    "\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=50, width=180)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/54405767\n",
    "def is_all_same(s):\n",
    "    a = s.to_numpy() # s.values (pandas<0.24)\n",
    "    return (a[0] == a).all()\n",
    "\n",
    "\n",
    "def entropy(df, feature_name, label_name):\n",
    "    # fetch the feature and target columns\n",
    "    feature = df[feature_name]\n",
    "    # label = df[label_name]\n",
    "    rows = len(df.index)\n",
    "\n",
    "    # get possible values of the feature\n",
    "    classes = feature.value_counts().to_dict()\n",
    "\n",
    "    entropy = 0\n",
    "\n",
    "    for key in classes:\n",
    "        # entropy calculation. same forumula from the slides,\n",
    "        # adapted for any number of feature classes instead of\n",
    "        # just true/false like in the assignment\n",
    "        val = classes[key]\n",
    "        proportion = val / rows\n",
    "        log = 0 if proportion == 0 else math.log(proportion, 2)\n",
    "\n",
    "        entropy -= proportion * log\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def id3(df, original, label_name, features):\n",
    "    root = {}\n",
    "\n",
    "    label = df[label_name]\n",
    "\n",
    "    # check exit conditions\n",
    "    label_all_same = is_all_same(label)\n",
    "    if label_all_same:\n",
    "        root[\"label\"] = label.iloc[0]\n",
    "        return root\n",
    "\n",
    "    lowest_entropy = 999999\n",
    "    best_feature = \"\"\n",
    "    # find the lowest entropy feature\n",
    "    for f in features:\n",
    "        e = entropy(df, f, label_name)\n",
    "        if (e < lowest_entropy):\n",
    "            lowest_entropy = e\n",
    "            best_feature = f\n",
    "\n",
    "    # if there's no more features to choose from we exit here\n",
    "    # with the value of the node as the mode of the target feature\n",
    "    if best_feature == \"\":\n",
    "        root[\"label\"] = df[label_name].mode()[0]\n",
    "        return root\n",
    "\n",
    "    # split\n",
    "    root[\"label\"] = best_feature\n",
    "    root[\"children\"] = {}\n",
    "    children = root[\"children\"]\n",
    "\n",
    "    # we need to get the classes of the feature from the whole\n",
    "    # dataset, not just the subset of examples we're working with.\n",
    "    # this way we can avoid some of the key-not-present errors\n",
    "    # when traversing the tree later on\n",
    "    classes = original[best_feature].unique()\n",
    "    for c in classes:\n",
    "        node = {}\n",
    "        examples = df[(df[best_feature] == c)]\n",
    "\n",
    "        if len(examples) == 0:\n",
    "            # if there's no examples for this class we end here with\n",
    "            # the mode of the target attribute\n",
    "            node[\"label\"] = df[label_name].mode()[0]\n",
    "        else:\n",
    "            new_features = features.copy()\n",
    "            new_features.remove(best_feature)\n",
    "            node = id3(examples, original, label_name, new_features)\n",
    "\n",
    "        children[c] = node\n",
    "\n",
    "    return root\n",
    "\n",
    "# perform a tree traversal for a single sample. Returns\n",
    "# the prediction for this sample, either 1 or 0\n",
    "def predict_sample(sample, tree):\n",
    "    curr_node = tree;\n",
    "    label = curr_node[\"label\"]\n",
    "    while 'children' in curr_node:\n",
    "        try:\n",
    "            val = sample[label]\n",
    "            child = curr_node[\"children\"][val]\n",
    "            curr_node = child\n",
    "            label = curr_node[\"label\"]\n",
    "        except KeyError:\n",
    "            print(\"validation error: key \" + str(val) + \" not found\")\n",
    "            return 0\n",
    "\n",
    "    return label\n",
    "\n",
    "# predicts all samples in the dataframe and returns the success rate\n",
    "def predict_all(df, tree, label_name):\n",
    "    correct = 0\n",
    "    error = 0\n",
    "    for index, row in df.iterrows():\n",
    "        real = row[label_name]\n",
    "        predicted = predict_sample(row, tree)\n",
    "        curr_err = abs(real - predicted)\n",
    "        error += curr_err\n",
    "\n",
    "        if real == predicted:\n",
    "            correct += 1\n",
    "\n",
    "    print(\"prediction rate: \" + str(correct / len(df) * 100) + \"%\")\n",
    "    return error / len(df)\n",
    "\n",
    "def tree_info(tree, features):\n",
    "    label = tree[\"label\"]\n",
    "    if 'children' in tree:\n",
    "\n",
    "        if label in features:\n",
    "            features[label] += 1\n",
    "        else:\n",
    "            features[label] = 1\n",
    "\n",
    "        children = tree[\"children\"]\n",
    "        max_depth = 0\n",
    "        for key in children:\n",
    "            (depth, features) = tree_info(children[key], features)\n",
    "            if depth > max_depth:\n",
    "                max_depth = depth\n",
    "        return (max_depth + 1, features)\n",
    "    else:\n",
    "        return (0, features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    label_name = \"weighted_average_vote\"\n",
    "\n",
    "    # Read the data\n",
    "    df = pd.read_csv(\"./content/combined_data_not_encoded.csv\")\n",
    "    # df = df.sample(n=1000, random_state=1337)\n",
    "\n",
    "\n",
    "    # Split 70/30 randomly\n",
    "    train = df.sample(frac=0.7, random_state=1337)\n",
    "    test = df.drop(train.index)\n",
    "\n",
    "    # Round the scores to the nearest 0.5 for training\n",
    "    train[label_name] *= 2\n",
    "    train = train.round({label_name: 0})\n",
    "    print(train.head())\n",
    "    print(test.head())\n",
    "    train[label_name] /= 2\n",
    "\n",
    "    # Get a list of features\n",
    "    features = list(df.columns.values)\n",
    "    features.remove(label_name)\n",
    "\n",
    "    # do not consider the continuous features when building\n",
    "    # the tree\n",
    "    to_remove = []\n",
    "    for f in features:\n",
    "        num_classes = len(train[f].value_counts())\n",
    "        # If the number of classes is more than 80 we consider it a continuous feature\n",
    "        if num_classes > 80:\n",
    "            to_remove.append(f)\n",
    "\n",
    "    for t in to_remove:\n",
    "        features.remove(t)\n",
    "\n",
    "    # build the tree\n",
    "    print(\"Selected features: \" + str(features))\n",
    "    print(\"Training...\")\n",
    "    tree = id3(train, train, label_name, features)\n",
    "\n",
    "    # optionally pretty print the tree\n",
    "    # pp.pprint(tree)\n",
    "\n",
    "    # write the tree to json\n",
    "    # with open('tree.json', 'w') as fp:\n",
    "    #     json.dump(tree, fp)\n",
    "    #     print(\"tree.json written\")\n",
    "\n",
    "    print(\"predicting...\")\n",
    "    rate = predict_all(test, tree, label_name)\n",
    "    print(\"MAE: \" + str(rate))\n",
    "    # print(\"prediction rate: \" + str(rate * 100) + \"%\")\n",
    "    (depth, features) = tree_info(tree, {})\n",
    "    print(\"max depth: \" + str(depth))\n",
    "    print(\"key counts in tree: \", features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
